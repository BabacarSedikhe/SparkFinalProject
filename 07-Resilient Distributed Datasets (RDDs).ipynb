{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "*Resilient Distributed Dataset* (RDD) est une collection d'objets immuables et distribués. Les RDD sont résilients ou tolérants aux pannes. Les collections d'objets partitionnés sont réparties dans un cluster, stockées en mémoire ou sur disque. Les RDD sont construits et manipulés grâce à un ensemble diversifié de transformations parallèles (*map*, *filter*, *join*) et d'actions (*count*, *collect*, *save*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un RDD peut être créé de plusieurs manières:\n",
    "* Paralléliser une collection\n",
    "* Lire des données à partir d'une source externe\n",
    "* Transformation d'un RDD existant\n",
    "* API de streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD  Operations\n",
    "\n",
    "###  Transformation (Lazy evaluation)\n",
    "Les transformations créent de nouvelles dataset à partir d'une dataset existante. Par exemple, *map* est une transformation qui passe chaque élément de l'ensemble de données via une fonction et renvoie un nouveau RDD représentant les résultats. Toutes les transformations dans Spark sont *lazy*, c'est à dire elles ne calculent pas le résultats automatiquement. Les transformations ne sont calculées que lorsqu'une action nécessite qu'un résultat soit renvoyé.  \n",
    "\n",
    "**Exemple de transformations**\n",
    "\n",
    "*map(),  flatMap(), filter(),  distinct(), intersection(), cartesian(), groupByKey(), coalesce(), mapPartitions(), reduceByKey(), repartition(), sortByKey(), partitionBy(), sample(), ...*  \n",
    "\n",
    "### Actions\n",
    "Les actions permettent de returner la valeur calculer sur le dataset vers le driver. Par exemple, *reduce* est une action qui agrège tous les éléments du RDD en utilisant une fonction et renvoie le résultat final vers le driver.\n",
    "\n",
    "**Exemple d'actions**\n",
    "*reduce(), collect(), count(), first(), take() countByKey(), takeSample(), foreach(), takeOrdered(), saveAsTextFile(), saveAsSequenceFile() ...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/rdd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Dans ce chapitre nous allons faire une introduction des RDD pour vous permettre de comprendre comment fonctionne Spark built-in. Depuis les récentes versions de Spark,les RDD sont désormais considérés dans API de bas niveau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parralleliser un collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Seq(0,1,2,3,4,5,6,7,8,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[Int] = Array(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "rdd.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_filter: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at filter at <console>:26\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_filter = rdd.filter(x => x % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[Int] = Array(0, 2, 4, 6, 8)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(x => x % 2 == 0).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[Int] = Array(0, 2, 4, 6, 8)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_filter.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_map: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[4] at map at <console>:26\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_map = rdd_filter.map(x => x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[Int] = Array(0, 4, 16, 36, 64)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_map.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: Int = 120\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = rdd_map.reduce((a, b) => a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sum_even_sqr: Int = 312960\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Foctorisation du code \n",
    "// piplining\n",
    "val sum_even_sqr =  rdd.filter(x => x % 2 == 0).map( x => x*x).map(x => x*x*x).reduce((x,y) => x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "texte: org.apache.spark.rdd.RDD[String] = Datasets/datasets/senegal.txt MapPartitionsRDD[9] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val texte= sc.textFile(\"Datasets/datasets/senegal.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Long = 5\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Numbre de ligne\n",
    "texte.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: String = \"Le Sénégal est un pays situé sur la côte ouest de l'Afrique et doté d'un héritage colonial français et de nombreuses attractions naturelles. \"\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texte.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing sur un fichier texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fistLines: Array[String] = Array(\"LE SÉNÉGAL EST UN PAYS SITUÉ SUR LA CÔTE OUEST DE L'AFRIQUE ET DOTÉ D'UN HÉRITAGE COLONIAL FRANÇAIS ET DE NOMBREUSES ATTRACTIONS NATURELLES. \", \"DAKAR, LA CAPITALE, COMPREND LE QUARTIER HISTORIQUE DE LA MÉDINA ET LE CÉLÈBRE MUSÉE THÉODORE MONOD, EXPOSANT DES ŒUVRES D'ART AFRICAIN. \")\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// convertir du texte en MAJ\n",
    "val fistLines = texte.map(line => line.toUpperCase()).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: String = \"LE SÉNÉGAL EST UN PAYS SITUÉ SUR LA CÔTE OUEST DE L'AFRIQUE ET DOTÉ D'UN HÉRITAGE COLONIAL FRANÇAIS ET DE NOMBREUSES ATTRACTIONS NATURELLES. \"\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fistLines(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le Sénégal est un pays situé sur la côte ouest de l'Afrique et doté d'un héritage colonial français et de nombreuses attractions naturelles. \n",
      "Le Sénégal est un pays dont le président est démocratiquement élu au suffrage universel directe.\n"
     ]
    }
   ],
   "source": [
    "// filtre les ligne commençant par L\n",
    "texte.filter(line => line.startsWith(\"L\")).take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LE SÉNÉGAL EST UN PAYS SITUÉ SUR LA CÔTE OUEST DE L'AFRIQUE ET DOTÉ D'UN HÉRITAGE COLONIAL FRANÇAIS ET DE NOMBREUSES ATTRACTIONS NATURELLES. \n",
      "LE SÉNÉGAL EST UN PAYS DONT LE PRÉSIDENT EST DÉMOCRATIQUEMENT ÉLU AU SUFFRAGE UNIVERSEL DIRECTE.\n"
     ]
    }
   ],
   "source": [
    "texte.map(line => line.toUpperCase()).filter(line => line.startsWith(\"L\")).take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[18] at distinct at <console>:26\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = texte.flatMap(line => line.split(' ')).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à\n",
      "sur\n",
      "française,\n",
      "directe.\n",
      "pour\n",
      "des\n",
      "ouest\n",
      "comprend\n",
      "d'un\n",
      "coloniale.\n"
     ]
    }
   ],
   "source": [
    "rdd.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "capitales: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[19] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val capitales = sc.parallelize(Seq(\"Abuja\",\"Accra\", \"Addis-Abeba\", \"Alger\", \"Dakar\", \"Banjul\", \n",
    "                                   \"Antananarivo\", \"Bamako\", \"Bissau\",\"Caire\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Array[String] = Array(Abuja, Accra, Addis-Abeba, Alger, Dakar)\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capitales.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pays: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[20] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pays =  sc.parallelize(Seq(\"Nigeria\", \"Ghana\", \"Éthiopie\", \"Algérie\", \"Sénégal\", \"Gambie\", \n",
    "                               \"Madagascar\",\"Mali\",\"Guinée-Bissau\", \"Égypte\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pays_capitales: org.apache.spark.rdd.RDD[(String, String)] = ZippedPartitionsRDD2[21] at zip at <console>:28\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pays_capitales = pays.zip(capitales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Nigeria,Abuja)\n",
      "(Ghana,Accra)\n",
      "(Éthiopie,Addis-Abeba)\n",
      "(Algérie,Alger)\n",
      "(Sénégal,Dakar)\n",
      "(Gambie,Banjul)\n",
      "(Madagascar,Antananarivo)\n",
      "(Mali,Bamako)\n",
      "(Guinée-Bissau,Bissau)\n",
      "(Égypte,Caire)\n"
     ]
    }
   ],
   "source": [
    " pays_capitales.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le famous wordcount avec Spark (RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_figaro: org.apache.spark.rdd.RDD[String] = datasets/ancien_figaro.txt MapPartitionsRDD[5] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_figaro =  sc.textFile(\"datasets/ancien_figaro.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_flatmap: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at flatMap at <console>:26\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_flatmap = rdd_figaro.flatMap(line => line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_map: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[7] at map at <console>:26\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_map = rdd_flatmap.map(w => (w, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordcounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at reduceByKey at <console>:26\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordcounts = rdd_map.reduceByKey((w1, w2) => w1 + w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at reduceByKey at <console>:26\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(journée,,1)\n",
      "(Ah!,19)\n",
      "(apostrophes,2)\n",
      "(destitué;,1)\n",
      "(budget!,2)\n",
      "(souvent,22)\n",
      "(combinaison,,1)\n",
      "(épigrammes.,2)\n",
      "(plaisir,,2)\n",
      "(resteront,1)\n"
     ]
    }
   ],
   "source": [
    "wordcounts.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordcounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[11] at reduceByKey at <console>:27\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Forme condensée\n",
    "val wordcounts = rdd_figaro.flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_+_)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(journée,,1)\n",
      "(Ah!,19)\n",
      "(apostrophes,2)\n",
      "(destitué;,1)\n",
      "(budget!,2)\n",
      "(souvent,22)\n",
      "(combinaison,,1)\n",
      "(épigrammes.,2)\n",
      "(plaisir,,2)\n",
      "(resteront,1)\n"
     ]
    }
   ],
   "source": [
    "wordcounts.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice  \n",
    "\n",
    "Reprendre le wordcount en supprimant les ponctuations, rendre le texte en minuscule, et ordonner le resultat en ordre décroissant des mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
